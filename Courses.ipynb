{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "   text = text.lower()\n",
    "   doc = word_tokenize(text)\n",
    "   doc = [word for word in doc if word not in stop_words]\n",
    "   doc = [word for word in doc if word.isalpha()]\n",
    "   return doc\n",
    "\n",
    "texts, y = ng20.data, ng20.target\n",
    "               \n",
    "corpus = [preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "sims = {'ng20': {}, 'snippets': {}}\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "corpus_gensim = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
    "lsi_index = MatrixSimilarity(lsi[corpus_tfidf])\n",
    "sims['ng20']['LSI'] = np.array([lsi_index[lsi[corpus_tfidf[i]]]\n",
    "                                for i in range(len(corpus))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import job data\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"/Users/ryancramer/Desktop/handshake_feed1.txt\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"/Users/ryancramer/Desktop/courses.txt\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "#train_texts = df4['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = df1['description']\n",
    "courses = df2['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "#from gensim.models import TfidfModel\n",
    "#from gensim.models import LsiModel\n",
    "#from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        alpha_tokens = [i for i in stopped_tokens if i.isalpha()]\n",
    "        # alpha tokens only\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in alpha_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jobs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "jobs = preprocess_data(jobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 585 ms, total: 39 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "courses = preprocess_data(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876\n",
      "8781\n"
     ]
    }
   ],
   "source": [
    "print(len(jobs))\n",
    "print(len(courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    jobs.append(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"/Users/ryancramer/Desktop/Jupyter Notebook Files/first_write.csv\")\n",
    "    \n",
    "Simple_Skills = df3[\"0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill = preprocess_data(Simple_Skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillss = list()\n",
    "for word in skill:\n",
    "    for skills in word:\n",
    "        if skills not in skillss:\n",
    "            skillss.append(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary23, doc_term_matrix23 = prepare_corpus(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobs simularity\n",
    "import gensim\n",
    "sims = {'jobs': {}, 'courses': {}}\n",
    "\n",
    "dictionary, doc_term_matrix = prepare_corpus(jobs)\n",
    "lsimodel = LsiModel(doc_term_matrix, num_topics=8, id2word = dictionary)  # train model\n",
    "#print(lsimodel.print_topics(num_topics=4, num_words=10))\n",
    "\n",
    "tfidf = TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "\n",
    "#\n",
    "#sims = gensim.similarities.Similarity(jobs,corpus_tfidf, num_features=len(dictionary))\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "lsi_index = MatrixSimilarity(lsimodel[corpus_tfidf])\n",
    "\n",
    "\n",
    "sims['jobs']['LSI'] = np.array([lsi_index[lsimodel[corpus_tfidf[i]]] for i in range(len(jobs))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#courses simularity\n",
    "dictionary_courses, doc_term_matrix_courses = prepare_corpus(courses)\n",
    "lsamodel_courses = LsiModel(doc_term_matrix_courses, num_topics=8, id2word = dictionary_courses)  # train model\n",
    "#print(lsamodel.print_topics(num_topics=4, num_words=10))\n",
    "\n",
    "\n",
    "tfidf2 = TfidfModel(doc_term_matrix_courses)\n",
    "corpus_tfidf2 = tfidf2[doc_term_matrix_courses]\n",
    "\n",
    "\n",
    "lsi_index_courses = MatrixSimilarity(lsamodel_courses[corpus_tfidf2])\n",
    "\n",
    "sims['courses']['LSI'] = np.array([lsi_index_courses[lsamodel_courses[corpus_tfidf2[i]]] for i in range(len(courses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = [w.lower() for w in (courses[0])]\n",
    "#print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "#print(query_doc_bow)\n",
    "query_doc_tf_idf = tfidf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    #num_topics = 5\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=10, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_coherence_values(dictionary, doc_term_matrix, jobs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,jobs,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#start,stop,step=2,10,1\n",
    "#plot_graph(jobs,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sims['courses']['LSI'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "#print(jobs[0])\n",
    "count = 0\n",
    "hold = 6491-876\n",
    "for word in jobs[0]:\n",
    "    if word in courses[(hold)]:\n",
    "        \n",
    "        count = count +1\n",
    "\n",
    "#print(count/len(jobs[15]))\n",
    "#print(courses[244])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspir\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'skill' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-19a4fa4c747b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skill' is not defined"
     ]
    }
   ],
   "source": [
    "for word in courses[hold]:\n",
    "    print(word)\n",
    "    if word in skill:\n",
    "        print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(courses[hold])\n",
    "print(jobs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(jobs[15])\n",
    "print(courses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860\n",
      "7226\n",
      "3926\n",
      "8606\n",
      "4158\n",
      "2646\n",
      "4116\n",
      "9115\n",
      "6106\n",
      "1135\n",
      "3536\n",
      "8220\n",
      "8346\n",
      "1280\n",
      "8684\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "r = np.argsort(sims['jobs']['LSI'][0])[::-1]\n",
    "#print(r)\n",
    "count = 0\n",
    "for word in r:\n",
    "    if word > 876:\n",
    "        print(word) \n",
    "        count = count + 1\n",
    "        if count == 20:\n",
    "            break\n",
    "\n",
    "#print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.argsort(sims['jobs']['LSI'][0])[::-1]\n",
    "#print(r)\n",
    "count = 0\n",
    "for word in r:\n",
    "    if word > 876:\n",
    "        print(word)\n",
    "        count = count + 1\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(i, X_sims, topn=None):\n",
    "    \"\"\"return the indices of the topn most similar documents with document i\n",
    "    given the similarity matrix X_sims\"\"\"\n",
    "               \n",
    "    r = np.argsort(X_sims[i])[::-1]\n",
    "    if r is None:\n",
    "        return r\n",
    "    else:\n",
    "        return r[:topn]\n",
    "               \n",
    "#LSI\n",
    "#print(sims['jobs']['LSI'][4])\n",
    "jobs_similar = most_similar(4, sims['jobs']['LSI'], 10)\n",
    "courses_similar = most_similar(4, sims['courses']['LSI'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs_similar[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(courses[467])\n",
    "print(courses[8687])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.argsort(sims['jobs']['LSI'][4])[::-1]\n",
    "print(r)\n",
    "print(r[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for job in jobs_similar:\n",
    "    print(job)\n",
    "\n",
    "#for job in courses_similar:\n",
    " #   print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs[4])\n",
    "print(jobs[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs_similar:\n",
    "    print(job)\n",
    "\n",
    "for job in courses_similar:\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(courses[0])\n",
    "print(courses[3310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, s in enumerate(sims['courses']['LSI']):\n",
    "    print(s, sims['courses']['LSI'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsi_index.print_topics(num_topics=4, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
