{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import job data\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"/Users/ryancramer/Desktop/handshake_feed1.txt\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"/Users/ryancramer/Desktop/courses.txt\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df_skill = pd.read_csv(\"/Users/ryancramer/Desktop/Jupyter Notebook Files/SkillsSet5.csv\")\n",
    "\n",
    "skill5 = list()\n",
    "df_skill = df_skill[\"0\"]\n",
    "for word in df_skill:\n",
    "    #print(word)\n",
    "    skill5.append(word)\n",
    "\n",
    "#train_texts = df4['description']\n",
    "jobs_text = df1['description']\n",
    "courses_text = df2['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        #sents = raw.split(\".\")\n",
    "        #tokens = tokenizer.tokenize(raw)\n",
    "        #text1 = []\n",
    "        #for sent in sents:\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "            # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            # stem tokens\n",
    "        alpha_tokens = [i for i in stopped_tokens if i.isalpha() and len(i)<14]\n",
    "            # alpha tokens only\n",
    "            #stemmed_tokens = [p_stemmer.stem(i) for i in alpha_tokens]\n",
    "            # add tokens to list\n",
    "        texts.append((\" \").join(alpha_tokens))\n",
    "        #texts.append(text1)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"/Users/ryancramer/opt/anaconda3/lib/python3.7/site-packages/en_core_web_lg/en_core_web_lg-2.3.1\", disable=['parser', 'ner'])\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docJ = nlp.pipe(jobs_text)\n",
    "docC = nlp.pipe(courses_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newwwwwwwww\n",
    "all_sents_processed = []\n",
    "for doc in docJ:\n",
    "    sentences = (list(doc.sents))\n",
    "    for sent in sentences:\n",
    "        all_sents_processed.append(sent)\n",
    "        \n",
    "for doc in docC:\n",
    "    sentences = (list(doc.sents))\n",
    "    for sent in sentences:\n",
    "        all_sents_processed.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 102 ms, total: 1.87 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "jobs_processed = preprocess_data(jobs_text)\n",
    "courses_processed = preprocess_data(courses_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171159\n",
      "CPU times: user 4.73 s, sys: 691 ms, total: 5.42 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = []\n",
    "for row in all_sents_processed:\n",
    "        corpus_hold = list()\n",
    "        for word in row:\n",
    "            #word = word.lower()\n",
    "            corpus_hold.append(str(word.lemma_.lower()))\n",
    "        corpus.append(corpus_hold)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 20 s, total: 3min 11s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "modelD = Doc2Vec(documents, window=5, min_count=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"D2VCorpus.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(doc_set):\n",
    "    texts = []\n",
    "    for i in doc_set:\n",
    "        text_with_grams = []\n",
    "        count = 1\n",
    "        holder = \"\"\n",
    "        words = i.split()\n",
    "        for word in words:\n",
    "            if count == 1:\n",
    "                holder = word\n",
    "            else:  \n",
    "                text_with_grams.append(holder + \" \" + word)\n",
    "                text_with_grams.append(holder)\n",
    "                if count == len(words):\n",
    "                    text_with_grams.append(word)\n",
    "                holder = word\n",
    "            count = count + 1\n",
    "        texts.append(text_with_grams)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(BETTER)\n",
    "def SkillConverter(jobs,skills):\n",
    "    texts = []\n",
    "    for job in jobs:\n",
    "        skill_text = []\n",
    "        skip_gram = False\n",
    "        for word in job:\n",
    "            #if skip_gram == True:\n",
    "                #skip_gram = False\n",
    "                #continue\n",
    "            #elif skip_gram == False and word in skills:\n",
    "                \n",
    "                #skill_text.append(word)\n",
    "                #skip_gram = True\n",
    "            if word in skills:\n",
    "                skill_text.append(word)\n",
    "        texts.append((\" \").join(skill_text))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 41s, sys: 8.51 s, total: 9min 49s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "jobs_ngram = ngram(jobs_processed)\n",
    "jobs_skills = SkillConverter(jobs_ngram,skill5)\n",
    "courses_ngram = ngram(courses_processed)\n",
    "courses_skills = SkillConverter(courses_ngram,skill5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp.pipe(jobs_skills)\n",
    "#doc4 = nlp.pipe(courses_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'employer' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f52d3416add1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'employer' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "for job in doc3:\n",
    "    print(modelD.wv[job])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advertise\n",
      "advertise\n",
      "advertise\n",
      "strut\n",
      "spre\n",
      "-PRON-\n",
      "-PRON-\n",
      "-PRON-\n",
      "zenwork\n",
      "administ\n",
      "ongoe\n",
      "ongoe\n",
      "advertise\n",
      "launder\n",
      "acuman\n",
      "-PRON-\n",
      "launder\n",
      "launder\n",
      "launder\n",
      "launder\n",
      "launder\n",
      "launder\n",
      "-PRON-\n",
      "ongoe\n",
      "strut\n",
      "strut\n",
      "-PRON-\n",
      "-PRON-\n",
      "advertise\n",
      "advertise\n",
      "advertise\n"
     ]
    }
   ],
   "source": [
    "#nOOOO\n",
    "job_sumsD = []\n",
    "for jobss in doc3:\n",
    "    count = 0\n",
    "    hold1 = np.empty(100)\n",
    "    for word in jobss:\n",
    "        #print(word.lemma_)\n",
    "        if word.lemma_ in modelD.wv.vocab:\n",
    "            hold1 = hold1 + modelD.wv[word.lemma_]\n",
    "        else:\n",
    "            print(word.lemma_)\n",
    "        count = count +1\n",
    "    if count != 0:\n",
    "        hold1 = hold1 / count\n",
    "    job_sumsD.append(hold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nOOO\n",
    "course_sumsD = []\n",
    "for jobss in doc4:\n",
    "    count = 0\n",
    "    hold1 = np.empty(100)\n",
    "    for word in jobss:\n",
    "        #print(word.lemma_)\n",
    "        if word.lemma_ in modelD.wv.vocab:\n",
    "            hold1 = hold1 + modelD.wv[word.lemma_]\n",
    "        else:\n",
    "            print(word.lemma_)\n",
    "        count = count +1\n",
    "    if count != 0:\n",
    "        hold1 = hold1 / count\n",
    "    course_sumsD.append(hold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sums = []\n",
    "for docum in jobs_skills:\n",
    "    job_sums.append(model.wv[docum])\n",
    "        \n",
    "course_sums = []\n",
    "for docum in courses_skills:\n",
    "    course_sums.append(model.wv[docum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "job_sums = []\n",
    "for jobss in jobs_skills:\n",
    "    count = 0\n",
    "    hold1 = np.empty(100)\n",
    "    for word in jobss.split():\n",
    "        #print(word)\n",
    "        hold1 = hold1 + model.wv[word]\n",
    "        count = count +1\n",
    "    if count != 0:\n",
    "        hold1 = hold1 / count\n",
    "    job_sums.append(hold1)\n",
    "\n",
    "course_sums = []\n",
    "for jobss in courses_skills:\n",
    "    count = 0\n",
    "    hold1 = np.empty(100)\n",
    "    for word in jobss.split():\n",
    "        #print(word)\n",
    "        hold1 = hold1 + model.wv[word]\n",
    "        count = count +1\n",
    "    if count != 0:\n",
    "        hold1 = hold1 / count\n",
    "    course_sums.append(hold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1 = 1-pairwise_distances(job_sums,course_sums, metric='cosine')\n",
    "sim2 = 1-pairwise_distances(job_sums,course_sums, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim1 = pd.DataFrame(sim1)\n",
    "df_sim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim2 = pd.DataFrame(sim2)\n",
    "df_sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
