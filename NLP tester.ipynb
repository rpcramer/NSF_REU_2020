{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting NLTK\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /Users/ryancramer/opt/anaconda3/lib/python3.7/site-packages (from NLTK) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /Users/ryancramer/opt/anaconda3/lib/python3.7/site-packages (from NLTK) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.5.14.tar.gz (696 kB)\n",
      "\u001b[K     |████████████████████████████████| 696 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /Users/ryancramer/opt/anaconda3/lib/python3.7/site-packages (from NLTK) (4.42.1)\n",
      "Building wheels for collected packages: NLTK, regex\n",
      "  Building wheel for NLTK (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NLTK: filename=nltk-3.5-py3-none-any.whl size=1434679 sha256=1808c5ba8342263311e0101fade107c6f82cb3a55c224fb40c014ce0a30f5f68\n",
      "  Stored in directory: /Users/ryancramer/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.5.14-cp37-cp37m-macosx_10_9_x86_64.whl size=287294 sha256=87454acf8097560061d4cd303d9434dc1d46bd5045a3685a085b46fa4ad4a331\n",
      "  Stored in directory: /Users/ryancramer/Library/Caches/pip/wheels/d4/9f/f1/da2c5e041f3f291659e2aca93cbe48355347b3e9e7c836ec08\n",
      "Successfully built NLTK regex\n",
      "Installing collected packages: regex, NLTK\n",
      "  Attempting uninstall: NLTK\n",
      "    Found existing installation: nltk 3.4.5\n",
      "    Uninstalling nltk-3.4.5:\n",
      "      Successfully uninstalled nltk-3.4.5\n",
      "Successfully installed NLTK-3.5 regex-2020.5.14\n"
     ]
    }
   ],
   "source": [
    "!pip install -U NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ryancramer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryancramer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ryancramer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train records: 3345\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read training data\n",
    "train_file = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/SMSSpamCollection.train\"\n",
    "train_data = pd.read_csv(train_file, sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "print(f'num train records: {len(train_data)}')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'you', 'doing?how', 'are', 'you?']\n"
     ]
    }
   ],
   "source": [
    "# simple example to split text with space\n",
    "text = \"What you doing?how are you?\"\n",
    "tokens = [t for t in text.split(' ')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'you', 'doing', '?', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# simple example to tokenize using NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"What you doing?how are you?\"\n",
    "# use lower case\n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('what', 1), ('you', 2), ('doing', 1), ('?', 2), ('how', 1), ('are', 1)])\n"
     ]
    }
   ],
   "source": [
    "# a simple example to calculate frequency\n",
    "import nltk\n",
    "\n",
    "freq = nltk.FreqDist(tokens)\n",
    "print(freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3345\n",
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "# tokenize training text\n",
    "train_texts = train_data['text']\n",
    "print(len(train_texts))\n",
    "train_text_tokens = [word_tokenize(text.lower())   for text in train_texts]\n",
    "# print the first example\n",
    "print(train_text_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'jurong', 'point', ',', 'crazy', '..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords_list)\n",
    "\n",
    "# remove stopwords in training tokens\n",
    "train_clean_tokens = list()\n",
    "for token_list in train_text_tokens:\n",
    "    new_token_list = list()\n",
    "    for token in token_list:\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "        new_token_list.append(token)\n",
    "        \n",
    "    train_clean_tokens.append(new_token_list)\n",
    "# print first record\n",
    "print(train_clean_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "increas\n"
     ]
    }
   ],
   "source": [
    "# a simple example\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "print(stemmer.stem('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'jurong', 'point', ',', 'crazi', '..', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'got', 'amor', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "# stemming for training data\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_stem_tokens = list()\n",
    "for token_list in train_clean_tokens:\n",
    "    new_token_list = [stemmer.stem(token)  for token in token_list]\n",
    "    train_stem_tokens.append(new_token_list)\n",
    "\n",
    "print(train_stem_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increas\n",
      "increase\n"
     ]
    }
   ],
   "source": [
    "# a simple example to compare stemming & lemmatization\n",
    "print(stemmer.stem('increases'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n"
     ]
    }
   ],
   "source": [
    "# more examples for lemmatization\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))  # play is verb\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\"))  # play is noun\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\"))  # play is adjective\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))  # play is adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test records: 1112\n",
      "0       Welcome to UK-mobile-date this msg is FREE giv...\n",
      "1       This is wishing you a great day. Moji told me ...\n",
      "2       Thanks again for your reply today. When is ur ...\n",
      "3       Sorry I flaked last night, shit's seriously go...\n",
      "4       He said i look pretty wif long hair wat. But i...\n",
      "                              ...                        \n",
      "1107    This is the 2nd time we have tried 2 contact u...\n",
      "1108                 Will ü b going to esplanade fr home?\n",
      "1109    Pity, * was in mood for that. So...any other s...\n",
      "1110    The guy did some bitching but I acted like i'd...\n",
      "1111                           Rofl. Its true to its name\n",
      "Name: text, Length: 1112, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# read test data\n",
    "test_file = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/SMSSpamCollection.test\"\n",
    "test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "print(f'num test records: {len(test_data)}')\n",
    "\n",
    "test_texts = test_data['text']\n",
    "print(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize testing text\n",
    "test_text_tokens = [word_tokenize(text.lower())   for text in test_texts]\n",
    "\n",
    "test_stem_tokens = list()\n",
    "for token_list in test_text_tokens:\n",
    "    new_token_list = list()\n",
    "    for token in token_list:\n",
    "        # ignore stopwords\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "        # stemming\n",
    "        new_token = stemmer.stem(token)\n",
    "        new_token_list.append(new_token)\n",
    "        \n",
    "    test_stem_tokens.append(new_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "               sparse=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token list to token dict\n",
    "train_tokens = [nltk.FreqDist(token_list) for token_list in train_stem_tokens]\n",
    "test_tokens = [nltk.FreqDist(token_list) for token_list in test_stem_tokens]\n",
    "\n",
    "# token dict to vector\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# define vectorizer\n",
    "feature_vectorizer = DictVectorizer()\n",
    "# learn token indices from training tokens\n",
    "feature_vectorizer.fit(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features shape: (3345, 6054)\n",
      "test features shape: (1112, 6054)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate feature vector for training:\n",
    "train_features = feature_vectorizer.transform(train_tokens)\n",
    "print(f'train features shape: {np.shape(train_features)}')\n",
    "# generate feature vector for testing:\n",
    "test_features = feature_vectorizer.transform(test_tokens)\n",
    "print(f'test features shape: {np.shape(test_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw labels for training, testing data\n",
    "\n",
    "train_labels = train_data['label']\n",
    "\n",
    "test_labels = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# learn encoding of labels\n",
    "label_encoder.fit(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3345,)\n",
      "[0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1]\n",
      "unique values: [0 1]\n",
      "unique values frequency: [2899  446]\n"
     ]
    }
   ],
   "source": [
    "# convert training labels into indices\n",
    "train_target = label_encoder.transform(train_labels)\n",
    "# shape of training label indices\n",
    "print(np.shape(train_target))\n",
    "# first 20 label indices\n",
    "print(train_target[0:20])\n",
    "\n",
    "# pay attention to unbalanced classes\n",
    "values, counts = np.unique(train_target, return_counts=True)\n",
    "print(f'unique values: {values}')\n",
    "print(f'unique values frequency: {counts}')\n",
    "\n",
    "# convert testing labels into indices\n",
    "test_target = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define classifier\n",
    "# what if we remove class_weight?\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "\n",
    "clf.fit(X=train_features, y=train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112,)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# get predicted label indices of testing data\n",
    "test_pred = clf.predict(X=test_features)\n",
    "\n",
    "print(np.shape(test_pred))\n",
    "print(test_pred[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam index is 1\n",
      "accuracy: 0.9811151079136691\n",
      "precision: 0.9769230769230769\n",
      "recall: 0.8758620689655172\n",
      "f1 score: 0.9236363636363637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# get spam label index\n",
    "spam_index = int(label_encoder.transform(['spam'])[0])\n",
    "print(f'spam index is {spam_index}')\n",
    "\n",
    "accuracy = accuracy_score(test_target, test_pred)\n",
    "f1 = f1_score(test_target, test_pred, pos_label=spam_index)\n",
    "precision = precision_score(test_target, test_pred, pos_label=spam_index)\n",
    "recall = recall_score(test_target, test_pred, pos_label=spam_index)\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
